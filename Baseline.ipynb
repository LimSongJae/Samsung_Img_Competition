{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# device 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (device)\n",
    "\n",
    "seed = 42 # seed 값 설정\n",
    "random.seed(seed) # 파이썬 난수 생성기\n",
    "os.environ['PYTHONHASHSEED'] = str(seed) # 해시 시크릿값 고정\n",
    "np.random.seed(seed) # 넘파이 난수 생성기\n",
    "\n",
    "torch.manual_seed(seed) # 파이토치 CPU 난수 생성기\n",
    "torch.backends.cudnn.deterministic = True # 확정적 연산 사용 설정\n",
    "torch.backends.cudnn.benchmark = False   # 벤치마크 기능 사용 해제\n",
    "torch.backends.cudnn.enabled = False        # cudnn 기능 사용 해제\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed) # 파이토치 GPU 난수 생성기\n",
    "    torch.cuda.manual_seed_all(seed) # 파이토치 멀티 GPU 난수 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12 #배경을 픽셀값 12로 간주\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = CustomDataset(csv_file='./data/train_source.csv', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "# 간단한 U-Net 모델 정의\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, 13, 1) # 12개 class + 1 background\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 초기화\n",
    "model = UNet().to(device)\n",
    "\n",
    "# loss function과 optimizer 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(20):  # 20 에폭 동안 학습합니다.\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(csv_file='./data/test.csv', transform=transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "        outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "        # batch에 존재하는 각 이미지에 대해서 반복\n",
    "        for pred in outputs:\n",
    "            pred = pred.astype(np.uint8)\n",
    "            pred = Image.fromarray(pred) # 이미지로 변환\n",
    "            pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "            pred = np.array(pred) # 다시 수치로 변환\n",
    "            # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "            for class_id in range(12):\n",
    "                class_mask = (pred == class_id).astype(np.uint8)\n",
    "                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                    mask_rle = rle_encode(class_mask)\n",
    "                    result.append(mask_rle)\n",
    "                else: # 마스크가 존재하지 않는 경우 -1\n",
    "                    result.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa174d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b30e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb888a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# device 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (device)\n",
    "\n",
    "seed = 42 # seed 값 설정\n",
    "random.seed(seed) # 파이썬 난수 생성기\n",
    "os.environ['PYTHONHASHSEED'] = str(seed) # 해시 시크릿값 고정\n",
    "np.random.seed(seed) # 넘파이 난수 생성기\n",
    "\n",
    "torch.manual_seed(seed) # 파이토치 CPU 난수 생성기\n",
    "torch.backends.cudnn.deterministic = True # 확정적 연산 사용 설정\n",
    "torch.backends.cudnn.benchmark = False   # 벤치마크 기능 사용 해제\n",
    "torch.backends.cudnn.enabled = False        # cudnn 기능 사용 해제\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed) # 파이토치 GPU 난수 생성기\n",
    "    torch.cuda.manual_seed_all(seed) # 파이토치 멀티 GPU 난수 생성기\n",
    "    \n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask[mask == 255] = 12 #배경을 픽셀값 12로 간주\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transform = A.Compose(\n",
    "        [   \n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = CustomDataset(csv_file='./data/train_source.csv', transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "    # U-Net의 기본 구성 요소인 Double Convolution Block을 정의합니다.\n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    # 간단한 U-Net 모델 정의\n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(UNet, self).__init__()\n",
    "            self.dconv_down1 = double_conv(3, 64)\n",
    "            self.dconv_down2 = double_conv(64, 128)\n",
    "            self.dconv_down3 = double_conv(128, 256)\n",
    "            self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "            self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "            self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "            self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "            self.conv_last = nn.Conv2d(64, 13, 1) # 12개 class + 1 background\n",
    "\n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool(conv3)   \n",
    "\n",
    "            x = self.dconv_down4(x)\n",
    "\n",
    "            x = self.upsample(x)        \n",
    "            x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "            x = self.dconv_up3(x)\n",
    "            x = self.upsample(x)        \n",
    "            x = torch.cat([x, conv2], dim=1)\n",
    "\n",
    "            x = self.dconv_up2(x)\n",
    "            x = self.upsample(x)        \n",
    "            x = torch.cat([x, conv1], dim=1)\n",
    "\n",
    "            x = self.dconv_up1(x)\n",
    "\n",
    "            out = self.conv_last(x)\n",
    "\n",
    "            return out\n",
    "\n",
    "    # model 초기화\n",
    "    model = UNet().to(device)\n",
    "\n",
    "    # loss function과 optimizer 정의\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(20):  # 20 에폭 동안 학습합니다.\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in tqdm(dataloader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')\n",
    "\n",
    "\n",
    "    test_dataset = CustomDataset(csv_file='./data/test.csv', transform=transform, infer=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        result = []\n",
    "        for images in tqdm(test_dataloader):\n",
    "            images = images.float().to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "            outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "            # batch에 존재하는 각 이미지에 대해서 반복\n",
    "            for pred in outputs:\n",
    "                pred = pred.astype(np.uint8)\n",
    "                pred = Image.fromarray(pred) # 이미지로 변환\n",
    "                pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "                pred = np.array(pred) # 다시 수치로 변환\n",
    "                # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "                for class_id in range(12):\n",
    "                    class_mask = (pred == class_id).astype(np.uint8)\n",
    "                    if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "                        mask_rle = rle_encode(class_mask)\n",
    "                        result.append(mask_rle)\n",
    "                    else: # 마스크가 존재하지 않는 경우 -1\n",
    "                        result.append(-1)\n",
    "\n",
    "\n",
    "    submit = pd.read_csv('./sample_submission.csv')\n",
    "    submit['mask_rle'] = result\n",
    "    submit\n",
    "\n",
    "    submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
